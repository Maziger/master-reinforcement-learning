{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Maziger/master-reinforcement-learning/blob/main/Notebooks/10_td_pred.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlrET7d3WOQ2"
      },
      "source": [
        "# Notebook pre steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "o0vEJAmyWKPb",
        "outputId": "fdc25cc6-6760-46af-ff32-5c73809d18b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dfply\n",
            "  Downloading dfply-0.3.3-py3-none-any.whl.metadata (452 bytes)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from dfply) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from dfply) (2.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->dfply) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->dfply) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->dfply) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->dfply) (1.17.0)\n",
            "Downloading dfply-0.3.3-py3-none-any.whl (612 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m612.4/612.4 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dfply\n",
            "Successfully installed dfply-0.3.3\n"
          ]
        }
      ],
      "source": [
        "#@title Installations\n",
        "\n",
        "# ALWAYS SAVE YOUR OWN COPY OF THIS NOTEBOOK: File > Save a copy in Drive\n",
        "# IF DANISH MENU DO: Hjælp > Engelsk version\n",
        "\n",
        "# To clear output do: Edit > Clear all outputs\n",
        "\n",
        "## Useful shortscuts\n",
        "# Run current cell: Cmd+Enter\n",
        "# Run current cell and goto next: Shift+Enter\n",
        "# Run selection (or line if no selection): Cmd+Shift+Enter\n",
        "\n",
        "# install missing packages\n",
        "!pip install dfply\n",
        "\n",
        "from dfply import *\n",
        "from plotnine import *\n",
        "import numpy as np  # RNG and vector ops\n",
        "import pandas as pd  # tabular outputs\n",
        "from IPython.display import Markdown\n",
        "# import json\n",
        "# from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "eRfJ8WsMofFf"
      },
      "outputs": [],
      "source": [
        "#@title MDP class\n",
        "\n",
        "import random\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "class MDP:\n",
        "    \"\"\"\n",
        "    A class representing a Markov Decision Process (MDP) using defaultdict structures.\n",
        "\n",
        "    This implementation includes state management, action specification, transition\n",
        "    probabilities, rewards, policies, and iterative algorithms for policy and value iteration.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initializes an empty MDP with model and state values.\n",
        "        \"\"\"\n",
        "        self.model = defaultdict(lambda: {\"pi\": None, \"actions\": defaultdict(dict)})\n",
        "        self.v = defaultdict(float)\n",
        "\n",
        "    def add_state_space(self, states):\n",
        "        \"\"\"\n",
        "        Adds states to the MDP.\n",
        "\n",
        "        Args:\n",
        "            states (list): List of state identifiers (strings or convertible to strings).\n",
        "        \"\"\"\n",
        "        for state in states:\n",
        "            _ = self.model[str(state)]\n",
        "        self.set_state_value()\n",
        "\n",
        "    def add_action_space(self, state_str, actions):\n",
        "        \"\"\"\n",
        "        Adds actions to a given state. Note you have to update the action\n",
        "        afterwards using `add_action`.\n",
        "\n",
        "        Args:\n",
        "            state_str (str): The state identifier.\n",
        "            actions (list): List of action identifiers.\n",
        "        \"\"\"\n",
        "        if not isinstance(state_str, str):\n",
        "            raise ValueError(\"State is not a sting!\")\n",
        "        if isinstance(actions, str):\n",
        "            # If it's a string, put it in a list to treat it as a single item\n",
        "            actions = [actions]\n",
        "        for action in actions:\n",
        "            # Initialize the action dictionary with 'pr' and 'r' keys\n",
        "            self.model[state_str][\"actions\"][str(action)] = {\"pr\": {}, \"r\": None}\n",
        "\n",
        "    def add_action(self, state_str, action_str, reward, pr):\n",
        "        \"\"\"\n",
        "        Adds a transition action with reward and transition probabilities.\n",
        "\n",
        "        Args:\n",
        "            state_str (str): State from which the action is taken.\n",
        "            action_str (str): Action identifier.\n",
        "            reward (float): Expected reward for taking the action.\n",
        "            pr (dict): Transition probabilities as {next_state: probability}.\n",
        "        \"\"\"\n",
        "        ## remove keys with zero trans pr\n",
        "        keys_to_remove = [key for key, value in pr.items() if value == 0]\n",
        "        for key in keys_to_remove:\n",
        "            del pr[key]\n",
        "        self.model[state_str][\"actions\"][action_str] = {\"r\": reward, \"pr\": pr}\n",
        "\n",
        "    def check(self, delta = 10*np.spacing(np.float64(1))):\n",
        "        \"\"\"\n",
        "        Performs checks on the built MDP model.\n",
        "\n",
        "        Verifies that transition probabilities sum to approximately 1.0 for each\n",
        "        state-action pair and checks for rewards less than the high_neg_reward.\n",
        "        Prints warnings if any issues are found.\n",
        "\n",
        "        Args:\n",
        "            delta (float, optional): Tolerance for the sum of transition probabilities. Defaults to 1e-10.\n",
        "        \"\"\"\n",
        "        ok = True\n",
        "        # Check if transition pr of an action sum to one\n",
        "        for state_label, state_content in self.model.items():\n",
        "            for action_label, action_content in state_content[\"actions\"].items():\n",
        "                pr = action_content[\"pr\"]\n",
        "                pr_sum = np.sum(list(pr.values()))\n",
        "                absdiff = np.abs(1-pr_sum)\n",
        "                if absdiff >= delta:\n",
        "                    print(f\"Warning: Transition probabilities for action '{action_label}' in state '{state_label}' do not sum to 1.0. Diff is: {absdiff}\")\n",
        "                    ok = False\n",
        "\n",
        "        # Check if there are states with no actions\n",
        "        for state_label, state_content in self.model.items():\n",
        "            if len(state_content[\"actions\"]) == 0:\n",
        "                print(f\"Warning: State '{state_label}' has no actions.\")\n",
        "                ok = False\n",
        "\n",
        "        # Check if all action transitions are to a state\n",
        "        states = list(self.model.keys())\n",
        "        for state_label, state_content in self.model.items():\n",
        "            for action_label, action_content in state_content[\"actions\"].items():\n",
        "                if not all(key in self.model for key in action_content['pr'].keys()):\n",
        "                    print(f\"Warning: Action '{action_label}' in state '{state_label}' has a transition to a non-existing state.\")\n",
        "                    ok = False\n",
        "        if ok:\n",
        "            print(\"All checks passed!\")\n",
        "\n",
        "\n",
        "    def normalize(self):\n",
        "        \"\"\"\n",
        "        Normalizes the transition probabilities for each state-action pair.\n",
        "        \"\"\"\n",
        "        for state_label, state_content in self.model.items():\n",
        "            for action_label, action_content in state_content[\"actions\"].items():\n",
        "                pr = action_content[\"pr\"]\n",
        "                pr_sum = np.sum(list(pr.values()))\n",
        "                for next_state_label, prob in pr.items():\n",
        "                    pr[next_state_label] = prob / pr_sum\n",
        "                action_content[\"pr\"] = pr\n",
        "\n",
        "    def set_state_value(self, states=None, value=0):\n",
        "        \"\"\"\n",
        "        Initializes or updates the value of states.\n",
        "\n",
        "        Args:\n",
        "            states (list, optional): List of state identifiers. Defaults to all states.\n",
        "            value (float, optional): Value to assign. Defaults to 0.\n",
        "        \"\"\"\n",
        "        states = states or list(self.model.keys())\n",
        "        for state in states:\n",
        "            self.v[state] = value\n",
        "\n",
        "    def set_random_deterministic_policy(self):\n",
        "        \"\"\"\n",
        "        Sets a random deterministic policy for each state.\n",
        "        \"\"\"\n",
        "        for state in self.model:\n",
        "            actions = list(self.model[state][\"actions\"].keys())\n",
        "            if actions:\n",
        "                chosen_action = random.choice(actions)\n",
        "                self.model[state][\"pi\"] = {chosen_action: 1}\n",
        "\n",
        "    def set_deterministic_policy(self, state_actions):\n",
        "        \"\"\"\n",
        "        Sets a deterministic policy from a state-action mapping.\n",
        "\n",
        "        Args:\n",
        "            state_actions (dict): Mapping {state: action}.\n",
        "        \"\"\"\n",
        "        for state, action in state_actions.items():\n",
        "            self.model[state][\"pi\"] = {action: 1}\n",
        "\n",
        "    def set_policy(self, states, pi):\n",
        "        \"\"\"\n",
        "        Sets a stochastic or deterministic policy for a list of states.\n",
        "\n",
        "        Args:\n",
        "            states (list): List of states to assign the policy.\n",
        "            pi (dict): Policy as {action: probability}.\n",
        "        \"\"\"\n",
        "        for state in states:\n",
        "            self.model[state][\"pi\"] = pi.copy()\n",
        "\n",
        "    def get_state_keys(self):\n",
        "        \"\"\"\n",
        "        Returns the list of state identifiers.\n",
        "\n",
        "        Returns:\n",
        "            list: List of state keys.\n",
        "        \"\"\"\n",
        "        return list(self.model.keys())\n",
        "\n",
        "    def get_action_keys(self, state):\n",
        "        \"\"\"\n",
        "        Returns the action identifiers for a given state.\n",
        "\n",
        "        Args:\n",
        "            state (str): State identifier.\n",
        "\n",
        "        Returns:\n",
        "            list: List of action keys.\n",
        "        \"\"\"\n",
        "        return list(self.model[state][\"actions\"].keys())\n",
        "\n",
        "    def get_action_info(self, state):\n",
        "        \"\"\"\n",
        "        Gets reward and transition probabilities for each action in a state.\n",
        "\n",
        "        Args:\n",
        "            state (str): State identifier.\n",
        "\n",
        "        Returns:\n",
        "            dict: Action information.\n",
        "        \"\"\"\n",
        "        return dict(self.model[state][\"actions\"])\n",
        "\n",
        "    def get_reward(self, state, action):\n",
        "        \"\"\"\n",
        "        Returns the reward for a given state-action pair.\n",
        "\n",
        "        Args:\n",
        "            state (str): State identifier.\n",
        "            action (str): Action identifier.\n",
        "\n",
        "        Returns:\n",
        "            float: Reward value.\n",
        "        \"\"\"\n",
        "        return self.model[state][\"actions\"][action][\"r\"]\n",
        "\n",
        "    def get_mdp_info(self):\n",
        "        \"\"\"\n",
        "        Returns information about the MDP.\n",
        "\n",
        "        Returns:\n",
        "            dict: The info.\n",
        "        \"\"\"\n",
        "        res = {}\n",
        "        res[\"states\"] = len(self.model)\n",
        "        res[\"actions\"] = sum([len(self.model[k][\"actions\"]) for k in self.model.keys()])\n",
        "        return res\n",
        "\n",
        "    def get_policy(self, add_state_values = False):\n",
        "        \"\"\"\n",
        "        Retrieves the current policy.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: Table of state, action, and probability.\n",
        "        \"\"\"\n",
        "        policy = []\n",
        "        for state in self.get_state_keys():\n",
        "            for action, prob in self.model[state][\"pi\"].items():\n",
        "                if not add_state_values:\n",
        "                    policy.append({\"state\": state, \"action\": action, \"pr\": prob})\n",
        "                else:\n",
        "                    policy.append({\"state\": state, \"action\": action, \"pr\": prob, \"v\": self.v[state]})\n",
        "        df = pd.DataFrame(policy)\n",
        "        df.set_index(\"state\")\n",
        "        return df\n",
        "\n",
        "    def get_state_values(self, states=None):\n",
        "        \"\"\"\n",
        "        Returns the current value of each state.\n",
        "\n",
        "        Args:\n",
        "            states (list, optional): Subset of states. Defaults to all.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: Table of state values.\n",
        "        \"\"\"\n",
        "        states = states or list(self.v.keys())\n",
        "        return pd.DataFrame([{\"state\": s, \"v\": self.v[s]} for s in states])\n",
        "\n",
        "    def get_mdp_matrices(self, high_neg_reward = -100000):\n",
        "        \"\"\"\n",
        "        Returns transition probability and reward matrices.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing:\n",
        "                p_mat (list): List of transition probability matrices.\n",
        "                r_mat (ndarray): Reward matrix.\n",
        "                states (list): List of state identifiers.\n",
        "                actions (list): List of action identifiers.\n",
        "        \"\"\"\n",
        "        states = self.get_state_keys()\n",
        "        actions = set(\n",
        "            action for state in states for action in self.get_action_keys(state)\n",
        "        )\n",
        "        actions = list(actions)\n",
        "        actions.sort()\n",
        "        p_mat = [pd.DataFrame(0.0, index=states, columns=states) for _ in actions]\n",
        "        for df in p_mat:\n",
        "            np.fill_diagonal(df.values, 1) # set default to transition to same state (so illigal actions work)\n",
        "        r_mat = pd.DataFrame(high_neg_reward, index=states, columns=actions)\n",
        "        for state in states:\n",
        "            for action in self.get_action_keys(state):\n",
        "                p_mat[actions.index(action)].at[state, state] = 0  # reset to 0 again (since action is not illigal)\n",
        "                pr = self.model[state][\"actions\"][action][\"pr\"]\n",
        "                r = self.model[state][\"actions\"][action][\"r\"]\n",
        "                r_mat.at[state, action] = r\n",
        "                for next_state, prob in pr.items():\n",
        "                    p_mat[actions.index(action)].at[state, next_state] = prob\n",
        "        p_mat = [m.to_numpy() for m in p_mat]  # convert to matrices\n",
        "        r_mat = r_mat.to_numpy()\n",
        "        return p_mat, r_mat, states, actions\n",
        "\n",
        "    def save_mdp(self, path: str | Path):\n",
        "        \"\"\"\n",
        "        Saves the MDP to a JSON file.\n",
        "\n",
        "        Args:\n",
        "            path (str | Path): Path to the JSON file.\n",
        "        \"\"\"\n",
        "        path = Path(path)\n",
        "        with path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(self.model, f, indent=2, sort_keys=True, ensure_ascii=False)\n",
        "\n",
        "    def load_mdp(self, path: str | Path):\n",
        "        \"\"\"\n",
        "        Loads the MDP from a JSON file.\n",
        "\n",
        "        Args:\n",
        "            path (str | Path): Path to the JSON file.\n",
        "        \"\"\"\n",
        "        with Path(path).open(\"r\", encoding=\"utf-8\") as f:\n",
        "            self.model = json.load(f)\n",
        "        self.check()\n",
        "\n",
        "    def bellman_calc(self, gamma, state, action):\n",
        "        \"\"\"\n",
        "        Computes Bellman update for a given state-action pair.\n",
        "\n",
        "        Args:\n",
        "            gamma (float): Discount factor.\n",
        "            state (str): State identifier.\n",
        "            action (str): Action identifier.\n",
        "\n",
        "        Returns:\n",
        "            float: Updated value.\n",
        "        \"\"\"\n",
        "        pr = self.model[state][\"actions\"][action][\"pr\"]\n",
        "        reward = self.model[state][\"actions\"][action][\"r\"]\n",
        "        return reward + gamma * sum(pr[s] * self.v[s] for s in pr)\n",
        "\n",
        "    def policy_eval(self, gamma, theta=1e-5, max_iter=10000, reset=True):\n",
        "        \"\"\"\n",
        "        Iteratively evaluates the current policy.\n",
        "\n",
        "        Args:\n",
        "            gamma (float): Discount factor.\n",
        "            theta (float): Convergence threshold.\n",
        "            max_iter (int): Max iterations.\n",
        "            reset (bool): Whether to reset state values to 0.\n",
        "        \"\"\"\n",
        "        if reset:\n",
        "            self.set_state_value()\n",
        "        for _ in range(max_iter):\n",
        "            delta = 0\n",
        "            for state in self.model:\n",
        "                v_old = self.v[state]\n",
        "                pi = self.model[state][\"pi\"]\n",
        "                value = sum(pi[a] * self.bellman_calc(gamma, state, a) for a in pi)\n",
        "                self.v[state] = value\n",
        "                delta = max(delta, abs(v_old - value))\n",
        "            if delta < theta:\n",
        "                break\n",
        "        else:\n",
        "            print(f\"Policy evaluation stopped at max iterations: {max_iter}\")\n",
        "\n",
        "    def policy_iteration(self, gamma, theta=1e-5, max_iter_eval=10000, max_iter_policy=100):\n",
        "        \"\"\"\n",
        "        Performs policy iteration with evaluation and improvement steps.\n",
        "\n",
        "        Args:\n",
        "            gamma (float): Discount factor.\n",
        "            theta (float): Convergence threshold.\n",
        "            max_iter_eval (int): Max iterations during policy evaluation.\n",
        "            max_iter_policy (int): Max policy improvement steps.\n",
        "        \"\"\"\n",
        "        self.set_random_deterministic_policy()\n",
        "        for i in range(max_iter_policy):\n",
        "            self.policy_eval(gamma, theta, max_iter_eval, reset=False)\n",
        "            stable = True\n",
        "            for state in self.model:\n",
        "                old_action = next(iter(self.model[state][\"pi\"]))\n",
        "                best_val, best_action = float(\"-inf\"), None\n",
        "                for action in self.get_action_keys(state):\n",
        "                    val = self.bellman_calc(gamma, state, action)\n",
        "                    if val > best_val:\n",
        "                        best_val = val\n",
        "                        best_action = action\n",
        "                self.model[state][\"pi\"] = {best_action: 1}\n",
        "                if best_action != old_action:\n",
        "                    stable = False\n",
        "            if stable:\n",
        "                break\n",
        "        else:\n",
        "            print(f\"Policy iteration stopped at max iterations: {max_iter_policy}\")\n",
        "        print(f\"Policy iteration finished in {i + 1} iterations.\")\n",
        "\n",
        "    def value_iteration(self, gamma, theta=1e-5, max_iter=10000, reset=True):\n",
        "        \"\"\"\n",
        "        Performs value iteration algorithm.\n",
        "\n",
        "        Args:\n",
        "            gamma (float): Discount factor.\n",
        "            theta (float): Convergence threshold.\n",
        "            max_iter (int): Max number of iterations.\n",
        "            reset (bool): Whether to reinitialize state values.\n",
        "        \"\"\"\n",
        "        self.set_random_deterministic_policy()\n",
        "        if reset:\n",
        "            self.set_state_value()\n",
        "        for i in range(max_iter):\n",
        "            delta = 0\n",
        "            for state in self.model:\n",
        "                v_old = self.v[state]\n",
        "                best_val, best_action = float(\"-inf\"), None\n",
        "                for action in self.get_action_keys(state):\n",
        "                    val = self.bellman_calc(gamma, state, action)\n",
        "                    if val > best_val:\n",
        "                        best_val = val\n",
        "                        best_action = action\n",
        "                self.v[state] = best_val\n",
        "                self.model[state][\"pi\"] = {best_action: 1}\n",
        "                delta = max(delta, abs(v_old - best_val))\n",
        "            if delta < theta:\n",
        "                break\n",
        "        else:\n",
        "            print(f\"Value iteration stopped at max iterations: {max_iter}\")\n",
        "        print(f\"Value iteration finished in {i + 1} iterations.\")\n",
        "\n",
        "    def get_steady_state_pr(self, as_dataframe = True, tol=1e-8):\n",
        "        \"\"\"\n",
        "        Calculates the steady-state probabilities for the MDP under the optimal policy.\n",
        "\n",
        "        Args:\n",
        "            as_dataframe (bool): Whether to return the result as a DataFrame, or otherwise as an array.\n",
        "\n",
        "        Returns:\n",
        "            if as_dataframe:\n",
        "                pd.DataFrame: A DataFrame with states and their steady-state probabilities.\n",
        "            else:\n",
        "                ndarray: An array of steady-state probabilities.\n",
        "        \"\"\"\n",
        "        state_labels_to_index = {label: index for index, label in enumerate(self.get_state_keys())}\n",
        "        num_states = len(state_labels_to_index)\n",
        "        transition_matrix = np.zeros((num_states, num_states))\n",
        "        policy = self.get_policy()\n",
        "        policy['s_idx'] = policy['state'].map(state_labels_to_index)\n",
        "        policy = policy.set_index(['s_idx', 'action'])\n",
        "        # calc transition matrix\n",
        "        for s_label in self.get_state_keys():\n",
        "            s_idx = state_labels_to_index[s_label]\n",
        "            action_rows = policy.loc[s_idx]\n",
        "            for action, row in action_rows.iterrows():\n",
        "                pi = row['pr']\n",
        "                a = self.model[s_label]['actions'][action]\n",
        "                for s_next_label, prob in a['pr'].items():\n",
        "                    s_next_idx = state_labels_to_index[s_next_label]\n",
        "                    transition_matrix[s_idx, s_next_idx] += prob * pi\n",
        "\n",
        "        transition_matrix.sum(axis=1)\n",
        "\n",
        "        ## calc steady state pr\n",
        "        # # alternative 1\n",
        "        # eigenvalues, left_eigenvectors = np.linalg.eig(transition_matrix.T)\n",
        "        # # Find the eigenvalue closest to 1\n",
        "        # closest_eigenvalue_index = np.abs(eigenvalues - 1).argmin()\n",
        "        # # Extract the corresponding left eigenvector\n",
        "        # steady_state_vector = left_eigenvectors[:, closest_eigenvalue_index]\n",
        "        # # Ensure the eigenvector contains real values and take the real part\n",
        "        # steady_state_vector = np.real(steady_state_vector)\n",
        "        # # Normalize the vector to sum to 1\n",
        "        # steady_state_vector = steady_state_vector / np.sum(steady_state_vector)\n",
        "        # # Handle potential negative values due to numerical precision by taking absolute value\n",
        "        # steady_state_vector = np.abs(steady_state_vector)\n",
        "        # steady_state_vector = steady_state_vector / np.sum(steady_state_vector)\n",
        "        # # Verify that the sum of the steady-state probabilities is approximately 1\n",
        "        # print(\"Sum of steady-state probabilities:\", np.sum(steady_state_vector))\n",
        "        # # Verify that all probabilities are non-negative\n",
        "        # print(\"Minimum steady-state probability:\", np.min(steady_state_vector))\n",
        "        # steady = steady_state_vector\n",
        "\n",
        "        # Alternative 2\n",
        "        eigvals, eigvecs = np.linalg.eig(transition_matrix.T)\n",
        "        steady = np.real(eigvecs[:, np.isclose(eigvals, 1)])\n",
        "        steady = steady[:,0]\n",
        "        sum(steady)\n",
        "        steady = steady/steady.sum()\n",
        "\n",
        "        # # Alternative 3 (slow)\n",
        "        # # Solve (P^T - I) d^T = 0 with sum(d)=1 by replacing one equation with the normalization\n",
        "        # A = transition_matrix.T - np.eye(num_states)\n",
        "        # b = np.zeros(num_states)\n",
        "        # A[-1, :] = 1.0\n",
        "        # b[-1] = 1.0\n",
        "        # # Least-squares for robustness\n",
        "        # d, *_ = np.linalg.lstsq(A, b, rcond=None)\n",
        "        # # Clean numerical noise\n",
        "        # d = np.maximum(d, 0)\n",
        "        # d = d / d.sum()\n",
        "\n",
        "        # abs(steady - steady_state_vector) < 0.00000001\n",
        "        # abs(d - steady_state_vector) < 0.00000001\n",
        "        # abs(steady - d) < 0.00000001\n",
        "\n",
        "        if abs(sum(steady) - 1) > tol:\n",
        "            raise ValueError(\"Steady state probabilities do not sum to 1.\")\n",
        "\n",
        "        if as_dataframe:\n",
        "            policy.reset_index(inplace=True)\n",
        "            policy['steady_pr'] = [steady[s_idx] for s_idx in policy['s_idx']]\n",
        "            return policy\n",
        "        else:\n",
        "            return steady\n",
        "\n",
        "# self = mdp\n",
        "# mdp.get_mdp_info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTjOLakAkCLV"
      },
      "source": [
        "# Temporal difference methods for prediction\n",
        "\n",
        "This notebook considers temporal difference (TD) learning one of the most fundamental concepts in reinforcement learning. TD learning is a combination of Monte Carlo (MC) and dynamic programming (DP) ideas. Like DP, TD update estimates based on other learned estimates, without waiting for a final outcome (bootstrap). That is, TD can learn on-line and do not need to wait until the whole sample-path is found. TD in general learn more efficiently than MC due to bootstrapping. In this module prediction using TD is considered.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqfijEcjk2bo"
      },
      "source": [
        "## A generic RL agent\n",
        "\n",
        "Let us now try to use RL and TD to estimate the value function for a policy. First, we define a generic RL agent that can be used for all environments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "szh8HtKZl2n6"
      },
      "outputs": [],
      "source": [
        "#@title Generic RL agent\n",
        "\n",
        "import math  # math helpers\n",
        "import random  # tie-breaking choices\n",
        "from collections import defaultdict  # lazy nested dicts\n",
        "from typing import Optional, List, Dict, Any  # typing\n",
        "\n",
        "import numpy as np  # vector ops and RNG\n",
        "import pandas as pd  # tabular data\n",
        "\n",
        "class RLAgent:\n",
        "    \"\"\"\n",
        "    Tabular RL agent with:\n",
        "      - per-state action dictionaries {'q': value, 'n': visits}\n",
        "      - behavior policy pi (dict action->prob)\n",
        "      - state value v and state visit counter n\n",
        "\n",
        "    Uses defaultdict so states/actions can be created lazily.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        # model[state] = dict with keys:\n",
        "        #   'pi': policy dict(action->prob)\n",
        "        #   'v': state value\n",
        "        #   'n': state visit count\n",
        "        #   'actions': dict(action -> {'q': float, 'n': int})\n",
        "        self.model: Dict[str, Dict[str, Any]] = defaultdict(\n",
        "            lambda: { # dict for a state\n",
        "                \"pi\": None,     # policy probabilities\n",
        "                \"v\": float(\"nan\"),  # state value\n",
        "                \"n\": 0,         # state visits\n",
        "                \"actions\": defaultdict(lambda: {\"q\": 0.0, \"n\": 0}),  # actions\n",
        "            }\n",
        "        )\n",
        "\n",
        "    # ----------------------------- helpers ------------------------------------\n",
        "\n",
        "    def add_states(self, states: List[str]) -> None:\n",
        "        \"\"\"Force creation of states (defaultdict makes them auto-create).\"\"\"  # eager create\n",
        "        for s in states:\n",
        "            _ = self.model[str(s)]  # touch to ensure creation\n",
        "\n",
        "    def add_state_action(self, s: str, a: str) -> None:\n",
        "        \"\"\"Ensure a state and a specific action exist.\"\"\"  # lazy create\n",
        "        _ = self.model[str(s)][\"actions\"][str(a)]  # touch to ensure creation\n",
        "\n",
        "    def add_actions(self, s: str, actions: List[str]) -> None:\n",
        "        \"\"\"Force creation of actions in state s.\"\"\"  # batch add\n",
        "        for a in actions:\n",
        "            _ = self.model[str(s)][\"actions\"][str(a)]  # touch to ensure creation\n",
        "\n",
        "    def add_states_and_actions(self, df: pd.DataFrame) -> None:\n",
        "        \"\"\"Bulk add (state, action) pairs from DataFrame with columns 's' and 'a'.\"\"\"  # bulk\n",
        "        for s, a in zip(df[\"s\"].astype(str), df[\"a\"].astype(str)):\n",
        "            _ = self.model[s][\"actions\"][a]  # touch-create\n",
        "\n",
        "    # ----------------------------- setters ------------------------------------\n",
        "\n",
        "    def set_action_value(self, value: float = 0.0) -> None:\n",
        "        \"\"\"Set q(s,a) to constant for all actions.\"\"\"  # initializer/reset\n",
        "        for s in self.model:\n",
        "            for a in self.model[s][\"actions\"]:\n",
        "                self.model[s][\"actions\"][a][\"q\"] = float(value)  # assign\n",
        "\n",
        "    def set_state_value(self,\n",
        "                        states: Optional[List[str]] = None,\n",
        "                        value: float = 0.0) -> None:\n",
        "        \"\"\"Set v(s) for given states (all if None).\"\"\"  # V setter\n",
        "        states = states or list(self.model.keys())\n",
        "        for s in states:\n",
        "            self.model[s][\"v\"] = float(value)  # assign\n",
        "\n",
        "    def set_action_ctr_value(self, ctr_value: int = 0) -> None:\n",
        "        \"\"\"Set all action counters to ctr_value.\"\"\"  # reset N(s,a)\n",
        "        for s in self.model:\n",
        "            for a in self.model[s][\"actions\"]:\n",
        "                self.model[s][\"actions\"][a][\"n\"] = int(ctr_value)  # assign\n",
        "\n",
        "    def set_state_ctr_value(self, ctr_value: int = 0) -> None:\n",
        "        \"\"\"Set all state visit counters to ctr_value.\"\"\"  # reset N(s)\n",
        "        for s in self.model:\n",
        "            self.model[s][\"n\"] = int(ctr_value)  # assign\n",
        "\n",
        "    def set_action_value_single(self,\n",
        "                                s: str,\n",
        "                                a: str,\n",
        "                                value: float = 0.0,\n",
        "                                ctr_value: int = 0) -> None:\n",
        "        \"\"\"Set q(s,a) and n(s,a) for a single state-action.\"\"\"  # direct set\n",
        "        _ = self.model[str(s)][\"actions\"][str(a)]  # ensure exists\n",
        "        self.model[s][\"actions\"][a][\"q\"] = float(value)  # set q\n",
        "        self.model[s][\"actions\"][a][\"n\"] = int(ctr_value)  # set n\n",
        "\n",
        "    def set_random_eps_greedy_policy(self, eps: float) -> None:\n",
        "        \"\"\"Set π(s) to random ε-greedy (random greedy action per state).\"\"\"  # init π\n",
        "        for s in self.model:\n",
        "            actions = list(self.model[s][\"actions\"].keys())  # available actions\n",
        "            if not actions:\n",
        "                self.model[s][\"pi\"] = None  # no policy if no actions\n",
        "                continue  # skip\n",
        "            base = eps / len(actions)  # exploration mass\n",
        "            pi = {a: base for a in actions}  # flat\n",
        "            a_star = random.choice(actions)  # random greedy pick\n",
        "            pi[a_star] += 1.0 - eps  # exploitation bump\n",
        "            self.model[s][\"pi\"] = pi  # store\n",
        "\n",
        "    def set_eps_greedy_policy(self, eps: float, states: List[str] | str) -> None:\n",
        "        \"\"\"Make policy epsilon-greedy w.r.t current q-values.\"\"\"  # improve π\n",
        "        states_list = [states] if isinstance(states, str) else list(states)\n",
        "        for s in states_list:\n",
        "            actions = list(self.model[s][\"actions\"].keys())\n",
        "            if not actions:\n",
        "                self.model[s][\"pi\"] = None  # no policy\n",
        "                continue  # skip\n",
        "            q_vals = np.array([self.model[s][\"actions\"][a][\"q\"] for a in actions])  # q-vector\n",
        "            max_mask = q_vals == q_vals.max()  # tie mask\n",
        "            idx = int(np.random.choice(np.flatnonzero(max_mask)))  # random tie\n",
        "            base = eps / len(actions)  # exploration mass\n",
        "            pi = {a: base for a in actions}  # flat init\n",
        "            pi[actions[idx]] += 1.0 - eps  # greedy bump\n",
        "            self.model[s][\"pi\"] = pi  # assign\n",
        "\n",
        "    def set_greedy_policy(self, states: Optional[List[str]] = None) -> None:\n",
        "        \"\"\"Set greedy deterministic policy from q-values.\"\"\"  # greedy π\n",
        "        states = states or list(self.model.keys())\n",
        "        for s in states:\n",
        "            actions = list(self.model[s][\"actions\"].keys())\n",
        "            if not actions:\n",
        "                self.model[s][\"pi\"] = None  # no policy\n",
        "                continue  # skip\n",
        "            q_vals = [self.model[s][\"actions\"][a][\"q\"] for a in actions]  # q list\n",
        "            best = actions[int(np.argmax(q_vals))]  # greedy idx\n",
        "            self.model[s][\"pi\"] = {best: 1.0}  # point mass\n",
        "\n",
        "    def set_policy(self, states: List[str], pi: Dict[str, float]) -> None:\n",
        "        \"\"\"Set π(s) explicitly for each s in states (probabilities need not be normalized).\"\"\"  # explicit π\n",
        "        total = sum(pi.values())  # sum\n",
        "        norm = {a: (p / total) for a, p in pi.items()} if total > 0 else {a: 0.0 for a in pi}  # normalize\n",
        "        for s in states:\n",
        "            self.model[s][\"pi\"] = dict(norm)  # copy in\n",
        "\n",
        "    # ----------------------------- getters ------------------------------------\n",
        "\n",
        "    def get_info(self):\n",
        "        \"\"\"\n",
        "        Returns information about the agent.\n",
        "\n",
        "        Returns:\n",
        "            dict: The info.\n",
        "        \"\"\"\n",
        "        res = {}\n",
        "        res[\"states\"] = len(self.model)\n",
        "        res[\"actions\"] = sum([len(self.model[k][\"actions\"]) for k in self.model.keys()])\n",
        "        return res\n",
        "\n",
        "    def get_state_keys(self) -> List[str]:\n",
        "        return list(self.model.keys())  # all states\n",
        "\n",
        "    def get_action_keys(self, s: str) -> List[str]:\n",
        "        return list(self.model[s][\"actions\"].keys())  # actions in s\n",
        "\n",
        "    def get_action_info(self, s: str) -> Dict[str, Dict[str, Any]]:\n",
        "        \"\"\"Return shallow copy of the actions dict for state s.\"\"\"  # inspection\n",
        "        return dict(self.model[s][\"actions\"])  # shallow copy\n",
        "\n",
        "    def get_state_value_q(self, s: str) -> float:\n",
        "        \"\"\"Compute v_pi(s) = sum_a pi(a|s) q(s,a).\"\"\"  # V from Q & π\n",
        "        pi = self.model[s][\"pi\"] or {}  # distribution\n",
        "        return float(sum(p * self.model[s][\"actions\"][a][\"q\"]\n",
        "                         for a, p in pi.items()))  # dot product\n",
        "\n",
        "    def get_state_values(self,\n",
        "                         states: Optional[List[str]] = None) -> pd.DataFrame:\n",
        "        \"\"\"Return DataFrame of (state, v). Uses dfply::mutate if available.\"\"\"  # tidy\n",
        "        states = states or list(self.model.keys())\n",
        "        df = pd.DataFrame({\"state\": states})  # seed\n",
        "        return pd.DataFrame({\n",
        "            \"state\": states,\n",
        "            \"v\": [self.model[s][\"v\"] for s in states],\n",
        "        })  # basic\n",
        "\n",
        "    def get_policy(self, states: Optional[List[str]] = None) -> pd.DataFrame:\n",
        "        \"\"\"Return DataFrame (state, action, pr) for current π.\"\"\"  # tidy π\n",
        "        states = states or list(self.model.keys())\n",
        "        rows = []  # collect\n",
        "        for s in states:\n",
        "            pi = self.model[s][\"pi\"] or {}  # distribution\n",
        "            for a, p in pi.items():\n",
        "                rows.append({\"state\": s, \"action\": a, \"pr\": float(p)})  # row\n",
        "        return pd.DataFrame(rows)  # frame\n",
        "\n",
        "    def get_state_action_q_mat(self) -> pd.DataFrame:\n",
        "        \"\"\"Return wide Q-matrix DataFrame (rows=states, cols=actions).\"\"\"  # matrix\n",
        "        states = list(self.model.keys())  # rows\n",
        "        actions = sorted({a for s in states for a in self.model[s][\"actions\"].keys()})  # unique cols\n",
        "        mat = pd.DataFrame(np.nan, index=states, columns=actions)  # init\n",
        "        for s in states:\n",
        "            for a, rec in self.model[s][\"actions\"].items():\n",
        "                mat.loc[s, a] = rec[\"q\"]  # fill\n",
        "        return mat  # matrix\n",
        "\n",
        "    def get_action_values(self,\n",
        "                          states: Optional[List[str]] = None) -> pd.DataFrame:\n",
        "        \"\"\"Return long-form DataFrame of q-values and counts.\"\"\"  # tidy Q\n",
        "        states = [states] if isinstance(states, str) else states\n",
        "        states = states or list(self.model.keys())\n",
        "        rows = []\n",
        "        for s in states:\n",
        "            for a, info in self.model[s][\"actions\"].items():\n",
        "                rows.append({\n",
        "                    \"state\": s,\n",
        "                    \"action\": a,\n",
        "                    \"q\": info[\"q\"],\n",
        "                    \"n\": info[\"n\"],\n",
        "                })\n",
        "        return pd.DataFrame(rows)  # frame\n",
        "\n",
        "    # ----------------------------- action selection ---------------------------\n",
        "\n",
        "    def get_action_ucb(self, s: str, coeff: float = 1.0) -> Optional[str]:\n",
        "        \"\"\"UCB1-like selection; updates n(s) and n(s,a).\"\"\"  # UCB\n",
        "        actions = list(self.model[s][\"actions\"].keys())  # available\n",
        "        if not actions:\n",
        "            return None  # no action\n",
        "        self.model[s][\"n\"] += 1  # visit state\n",
        "        qv = np.array([self.model[s][\"actions\"][a][\"q\"] for a in actions], dtype=float)  # q\n",
        "        na = np.array([max(1, self.model[s][\"actions\"][a][\"n\"]) for a in actions], dtype=float)  # counts\n",
        "        ns = float(self.model[s][\"n\"])  # state count\n",
        "        bonus = coeff * np.sqrt(np.log(ns + 1e-4) / na)  # exploration term\n",
        "        idx = int(np.argmax(qv + bonus))  # argmax\n",
        "        a = actions[idx]  # pick\n",
        "        self.model[s][\"actions\"][a][\"n\"] += 1  # visit action\n",
        "        return a  # chosen\n",
        "\n",
        "    def get_action_eg(self, s: str, eps: float) -> str:\n",
        "        \"\"\"Epsilon-greedy action selection (increments counters).\"\"\"  # ε-greedy\n",
        "        self.model[s][\"n\"] += 1  # visit state\n",
        "        actions = list(self.model[s][\"actions\"].keys())  # list\n",
        "        q = np.array([self.model[s][\"actions\"][a][\"q\"] for a in actions])  # q-values\n",
        "        max_mask = q == q.max()  # ties\n",
        "        idx_greedy = int(np.random.choice(np.flatnonzero(max_mask)))  # random tie\n",
        "        probs = np.full(len(actions), eps / len(actions), dtype=float)  # base mass\n",
        "        probs[idx_greedy] += 1.0 - eps  # greedy bump\n",
        "        idx = int(np.random.choice(np.arange(len(actions)), p=probs))  # sample\n",
        "        a = actions[idx]  # chosen action\n",
        "        self.model[s][\"actions\"][a][\"n\"] += 1  # visit action\n",
        "        return a  # return\n",
        "\n",
        "    def get_action_pi(self, s: str) -> Optional[str]:\n",
        "        \"\"\"Sample an action from stored pi(a|s) (increments counters).\"\"\"  # sample π\n",
        "        self.model[s][\"n\"] += 1  # visit state\n",
        "        pi = self.model[s][\"pi\"] or {}  # distribution\n",
        "        if not pi:\n",
        "            return None  # no policy\n",
        "        actions, probs = zip(*pi.items())  # unpack\n",
        "        probs = np.array(probs, dtype=float)  # array\n",
        "        probs /= probs.sum() if probs.sum() > 0 else 1.0  # normalize\n",
        "        a = str(np.random.choice(list(actions), p=probs))  # draw\n",
        "        self.model[s][\"actions\"][a][\"n\"] += 1  # visit action\n",
        "        return a  # chosen\n",
        "\n",
        "    def get_max_action_value(self, s: str) -> float:\n",
        "        \"\"\"Return max_a Q(s,a).\"\"\"  # convenience\n",
        "        q = [rec[\"q\"] for rec in self.model[s][\"actions\"].values()]  # list\n",
        "        return float(max(q)) if q else float(\"nan\")  # handle empty\n",
        "\n",
        "    def get_exp_action_value(self, s: str) -> float:\n",
        "        \"\"\"Return E_{a~π}[Q(s,a)] under current π(s).\"\"\"  # expectation\n",
        "        pi = self.model[s][\"pi\"] or {}  # distribution\n",
        "        return float(sum(p * self.model[s][\"actions\"][a][\"q\"] for a, p in pi.items()))  # dot\n",
        "\n",
        "    # ----------------------------- learning -----------------------------------\n",
        "\n",
        "    def policy_eval_td0(self,\n",
        "                        env: Any,\n",
        "                        gamma: float = 1.0,\n",
        "                        alpha: float = 0.1,\n",
        "                        max_e: int = 1000,\n",
        "                        max_el: int = 10000,\n",
        "                        reset: bool = True,\n",
        "                        states: Optional[List[str]] = None) -> None:\n",
        "        \"\"\"\n",
        "        TD(0) policy evaluation of V(s). The environment used must implement:\n",
        "        get_time_step_data(s, a) -> {'r': float, 'sN': next_state or None (terminal)}.\n",
        "\n",
        "        Args:\n",
        "            env: Environment with get_time_step_data method.\n",
        "            gamma: The discount factor.\n",
        "            alpha: Step-size parameter\n",
        "            max_e: Maximum number of iterations (episodes)\n",
        "            max_el: Maximum episode length.\n",
        "            reset: Reset action-values, state and action counters to 0.\n",
        "            states: Starting states. For each iteration, generate\n",
        "                an episode for each state. If `None` uses all states.\n",
        "        \"\"\"\n",
        "        if reset:\n",
        "            self.set_state_value(value=0.0)  # V=0\n",
        "        starts = states or self.get_state_keys()  # candidate starts\n",
        "        for _ in range(max_e):  # episodes\n",
        "            s = random.choice(starts)  # start\n",
        "            for i in range(max_el):  # steps\n",
        "                a = self.get_action_pi(s)  # act under π\n",
        "                if a is None:  # no policy\n",
        "                    break  # abort\n",
        "                dat = env.get_time_step_data(s, a)  # env step\n",
        "                r, sN = dat[\"r\"], dat[\"sN\"]  # unpack\n",
        "                if sN is None:  # terminal\n",
        "                    break  # end\n",
        "                old_v = self.model[s][\"v\"]  # current V\n",
        "                td_target = r + gamma * self.model[sN][\"v\"]  # target\n",
        "                self.model[s][\"v\"] = old_v + alpha * (td_target - old_v)  # update\n",
        "                s = sN  # advance\n",
        "            if i + 1 == max_el:  # cap hit\n",
        "                break  # stop\n",
        "\n",
        "    def policy_eval_mc(self,\n",
        "                       env: Any,\n",
        "                       gamma: float = 1.0,\n",
        "                       theta: float = 0.1,\n",
        "                       min_ite: int = 100,\n",
        "                       max_ite: int = 2000,\n",
        "                       reset: bool = True,\n",
        "                       states: Optional[List[str]] = None,\n",
        "                       verbose: bool = False) -> None:\n",
        "        \"\"\"\n",
        "        Every-visit Monte Carlo evaluation of V(s).\n",
        "        Env must implement: get_episode_pi(agent, s0) -> DataFrame with columns ['s','a','r'].  # contract\n",
        "        \"\"\"  # doc\n",
        "        if reset:\n",
        "            self.set_state_value(value=0.0)  # V=0\n",
        "            self.set_action_ctr_value(0)  # reset N(s,a)\n",
        "            self.set_state_ctr_value(0)  # reset N(s)\n",
        "        starts = states or self.get_state_keys()  # start set\n",
        "        for ite in range(1, max_ite + 1):  # iterations\n",
        "            delta = 0.0  # max change\n",
        "            for s0 in starts:  # episode per start\n",
        "                df = env.get_episode_pi(self, s0)  # generate under π\n",
        "                if df is None or len(df) == 0:\n",
        "                    continue  # skip\n",
        "                if verbose:\n",
        "                    df['g'] = np.nan\n",
        "                    df['n_s'] = np.nan\n",
        "                    df['old_v'] = np.nan\n",
        "                    df['new_v'] = np.nan\n",
        "                g = 0.0  # return accumulator\n",
        "                for i in range(len(df) - 1, -1, -1):  # reverse pass\n",
        "                    s = str(df.iloc[i][\"s\"])  # state\n",
        "                    r = float(df.iloc[i][\"r\"])  # reward\n",
        "                    g = r + gamma * g  # return update\n",
        "                    n_s = max(1, self.model[s][\"n\"])  # denom\n",
        "                    old_v = self.model[s][\"v\"]  # prev\n",
        "                    step = 1.0 / n_s  # 1/N schedule\n",
        "                    self.model[s][\"v\"] = old_v + step * (g - old_v)  # update\n",
        "                    delta = max(delta, abs(old_v - self.model[s][\"v\"]))  # track\n",
        "                    if verbose:\n",
        "                        df.at[i,\"g\"] = g\n",
        "                        df.at[i,\"n_s\"] = n_s\n",
        "                        df.at[i,\"old_v\"] = old_v\n",
        "                        df.at[i,\"new_v\"] = self.model[s][\"v\"]\n",
        "                if verbose:\n",
        "                    print(\"Episode:\")\n",
        "                    print(df)  # trace\n",
        "            if delta < theta and ite >= min_ite:  # convergence\n",
        "                break  # stop\n",
        "        if ite == max_ite:\n",
        "            print(f\"Policy eval algorithm stopped at max iterations allowed: {max_ite}\")  # warn\n",
        "        print(f\"Policy eval algorithm finished in {ite} iterations.\")  # info\n",
        "\n",
        "    def gpi_on_policy_mc(self,\n",
        "                         env: Any,\n",
        "                         gamma: float = 1.0,\n",
        "                         theta: float = 0.1,\n",
        "                         min_ite: int = 100,\n",
        "                         max_ite: int = 1000,\n",
        "                         reset: bool = True,\n",
        "                         states: Optional[List[str]] = None,\n",
        "                         eps: float = 0.1,\n",
        "                         verbose: bool = False) -> None:\n",
        "        \"\"\"\n",
        "        On-policy GPI via Every-Visit MC control on Q(s,a).\n",
        "        Env must implement: get_episode(agent, s0, eps) -> DataFrame ['s','a','r']\n",
        "        and update visit counters.\n",
        "\n",
        "        Args:\n",
        "            env (Any): Environment with get_episode method.\n",
        "            gamma (float, optional): Discount factor. Defaults to 1.0.\n",
        "            theta (float, optional): Convergence threshold. Defaults to 0.1.\n",
        "            min_ite (int, optional): Minimum number of iterations. Defaults to 100.\n",
        "            max_ite (int, optional): Maximum number of iterations. Defaults to 1000.\n",
        "            reset (bool, optional): Reset action-values, state and action counters to 0.\n",
        "            states (list, optional): Starting states. For each iteration, generate\n",
        "                an episiode for each state. If `None uses all states.\n",
        "            eps (float, optional): Epsilon for eps-greedy policy. Defaults to 0.1.\n",
        "            verbose (bool, optional): Print episode info. Defaults to False.\n",
        "        \"\"\"\n",
        "        if reset:\n",
        "            self.set_action_value(0.0)  # Q=0\n",
        "            self.set_action_ctr_value(0)  # reset N(s,a)\n",
        "            self.set_state_ctr_value(0)  # reset N(s)\n",
        "        self.set_eps_greedy_policy(eps, self.get_state_keys())  # init π_ε\n",
        "        starts = states or self.get_state_keys()  # start pool\n",
        "        for ite in range(1, max_ite + 1):  # iterations\n",
        "            delta = 0.0  # track |ΔV|\n",
        "            for s0 in starts:  # episode per start\n",
        "                df = env.get_episode(self, s0, eps)  # behavior inside env\n",
        "                if verbose:\n",
        "                    df['g'] = np.nan\n",
        "                    df['n_sa'] = np.nan\n",
        "                    df['old_q'] = np.nan\n",
        "                    df['step'] = np.nan\n",
        "                    df['new_q'] = np.nan\n",
        "                    df['old_v'] = np.nan\n",
        "                    df['new_v'] = np.nan\n",
        "                if df is None or len(df) == 0:\n",
        "                    continue  # skip\n",
        "                g = 0.0  # return\n",
        "                for i in range(len(df) - 1, -1, -1):  # reverse sweep\n",
        "                    s = str(df.iloc[i][\"s\"])  # state\n",
        "                    a = str(df.iloc[i][\"a\"])  # action\n",
        "                    r = float(df.iloc[i][\"r\"])  # reward\n",
        "                    g = r + gamma * g  # return update\n",
        "                    # step size: (1 / n_sa) ** 0.5 as in R  # schedule\n",
        "                    n_sa = max(1, self.model[s][\"actions\"][a][\"n\"])  # visits\n",
        "                    old_q = self.model[s][\"actions\"][a][\"q\"]  # prev Q\n",
        "                    old_v = self.get_state_value_q(s)  # V before update\n",
        "                    step = (1.0 / n_sa) ** 0.5  # step-size\n",
        "                    new_q = old_q + step * (g - old_q)  # MC update\n",
        "                    self.model[s][\"actions\"][a][\"q\"] = new_q  # MC update\n",
        "                    self.set_eps_greedy_policy(eps, [s])  # improve π(s)\n",
        "                    new_v = self.get_state_value_q(s)  # V after\n",
        "                    delta = max(delta, abs(old_v - new_v))  # track\n",
        "                    if verbose:\n",
        "                        df.at[i,\"g\"] = g\n",
        "                        df.at[i,\"n_sa\"] = n_sa\n",
        "                        df.at[i,\"old_q\"] = old_q\n",
        "                        df.at[i,\"step\"] = step\n",
        "                        df.at[i,\"new_q\"] = new_q\n",
        "                        df.at[i,\"old_v\"] = old_v\n",
        "                        df.at[i,\"new_v\"] = new_v\n",
        "                if verbose:\n",
        "                    print(\"Episode:\")\n",
        "                    print(df)\n",
        "\n",
        "            if delta < theta and ite >= min_ite:\n",
        "                break  # stop\n",
        "        if ite == max_ite:\n",
        "            print(f\"GPI algorithm stopped at max iterations allowed: {max_ite}\")\n",
        "        print(f\"GPI algorithm finished in {ite} iterations.\")  # info\n",
        "\n",
        "    def gpi_on_policy_sarsa(self,\n",
        "                            env: Any,\n",
        "                            gamma: float = 1.0,\n",
        "                            max_e: int = 1000,\n",
        "                            max_el: int = 10000,\n",
        "                            reset: bool = True,\n",
        "                            states: Optional[List[str]] = None,\n",
        "                            eps: float = 0.1,\n",
        "                            alpha: float = 0.1,\n",
        "                            verbose: bool = False) -> None:\n",
        "        \"\"\"\n",
        "        On-policy SARSA with fixed α.\n",
        "        Env must implement: get_time_step_data(s, a) -> {'r': float, 'sN': next_state or None}.  # contract\n",
        "        \"\"\"  # doc\n",
        "        if reset:\n",
        "            self.set_action_value(0.0)  # Q=0\n",
        "            self.set_action_ctr_value(0)  # counts\n",
        "        self.set_eps_greedy_policy(eps, self.get_state_keys())  # init π_ε\n",
        "        starts = states or self.get_state_keys()  # start pool\n",
        "        for _ in range(max_e):  # episodes\n",
        "            s = random.choice(starts)  # pick start\n",
        "            a = self.get_action_pi(s)  # first action under π\n",
        "            for i in range(max_el):  # steps\n",
        "                if a is None:\n",
        "                    break  # no action available\n",
        "                dat = env.get_time_step_data(s, a)  # env step\n",
        "                r, sN = dat[\"r\"], dat[\"sN\"]  # unpack\n",
        "                if sN is None:\n",
        "                    break  # terminal\n",
        "                aN = self.get_action_pi(sN)  # next action\n",
        "                old_q = self.model[s][\"actions\"][a][\"q\"]  # current\n",
        "                target = r + gamma * (self.model[sN][\"actions\"][aN][\"q\"] if aN is not None else 0.0)  # SARSA target\n",
        "                self.model[s][\"actions\"][a][\"q\"] = old_q + alpha * (target - old_q)  # update\n",
        "                if verbose:\n",
        "                    print(f\"(s,a,r,sN,aN)=({s},{a},{r},{sN},{aN}) oldQ={old_q} newQ={self.model[s]['actions'][a]['q']}\")  # trace\n",
        "                self.set_eps_greedy_policy(eps, [s])  # local improve\n",
        "                s, a = sN, aN  # advance\n",
        "            if i + 1 == max_el:\n",
        "                break  # cap hit\n",
        "        print(\"GPI/SARSA finished.\")  # info\n",
        "\n",
        "    def gpi_off_policy_q_learning(self,\n",
        "                                  env: Any,\n",
        "                                  gamma: float = 1.0,\n",
        "                                  max_e: int = 1000,\n",
        "                                  max_el: int = 10000,\n",
        "                                  reset: bool = True,\n",
        "                                  states: Optional[List[str]] = None,\n",
        "                                  eps: float = 0.1,\n",
        "                                  alpha: float = 0.1,\n",
        "                                  verbose: bool = False) -> None:\n",
        "        \"\"\"\n",
        "        Off-policy Q-learning with behavior π_ε and greedy target.  # control\n",
        "        \"\"\"  # doc\n",
        "        if reset:\n",
        "            self.set_action_value(0.0)  # Q=0\n",
        "            self.set_action_ctr_value(0)  # counts\n",
        "        self.set_eps_greedy_policy(eps, self.get_state_keys())  # behavior π_ε\n",
        "        starts = states or self.get_state_keys()  # start pool\n",
        "        for _ in range(max_e):  # episodes\n",
        "            s = random.choice(starts)  # start\n",
        "            for i in range(max_el):  # steps\n",
        "                a = self.get_action_pi(s)  # behavior action\n",
        "                if a is None:\n",
        "                    break  # no action\n",
        "                dat = env.get_time_step_data(s, a)  # env step\n",
        "                r, sN = dat[\"r\"], dat[\"sN\"]  # unpack\n",
        "                if sN is None:\n",
        "                    break  # terminal\n",
        "                q_next = [rec[\"q\"] for rec in self.model[sN][\"actions\"].values()]  # next Qs\n",
        "                max_q = max(q_next) if q_next else 0.0  # greedy target\n",
        "                old_q = self.model[s][\"actions\"][a][\"q\"]  # current\n",
        "                self.model[s][\"actions\"][a][\"q\"] = old_q + alpha * (r + gamma * max_q - old_q)  # update\n",
        "                if verbose:\n",
        "                    print(f\"(s,a,r,sN)=({s},{a},{r},{sN}) oldQ={old_q} maxQ={max_q} newQ={self.model[s]['actions'][a]['q']}\")  # trace\n",
        "                self.set_eps_greedy_policy(eps, [s])  # refresh behavior at s\n",
        "                s = sN  # advance\n",
        "            if i + 1 == max_el:\n",
        "                break  # cap\n",
        "        self.set_greedy_policy()  # finalize with greedy π\n",
        "        print(\"GPI/Q-learning finished.\")  # info\n",
        "\n",
        "    def gpi_on_policy_exp_sarsa(self,\n",
        "                                env: Any,\n",
        "                                gamma: float = 1.0,\n",
        "                                max_e: int = 1000,\n",
        "                                max_el: int = 10000,\n",
        "                                reset: bool = True,\n",
        "                                states: Optional[List[str]] = None,\n",
        "                                eps: float = 0.1,\n",
        "                                alpha: float = 0.1,\n",
        "                                verbose: bool = False) -> None:\n",
        "        \"\"\"\n",
        "        On-policy Expected SARSA with fixed α.  # control\n",
        "        \"\"\"  # doc\n",
        "        if reset:\n",
        "            self.set_action_value(0.0)  # Q=0\n",
        "            self.set_action_ctr_value(0)  # counts\n",
        "        self.set_eps_greedy_policy(eps, self.get_state_keys())  # init π_ε\n",
        "        starts = states or self.get_state_keys()  # start pool\n",
        "        for _ in range(max_e):  # episodes\n",
        "            s = random.choice(starts)  # start\n",
        "            for i in range(max_el):  # steps\n",
        "                a = self.get_action_pi(s)  # act under π\n",
        "                if a is None:\n",
        "                    break  # no action\n",
        "                dat = env.get_time_step_data(s, a)  # env step\n",
        "                r, sN = dat[\"r\"], dat[\"sN\"]  # unpack\n",
        "                if sN is None:\n",
        "                    break  # terminal\n",
        "                exp_q = self.get_exp_action_value(sN)  # expectation under π(sN)\n",
        "                old_q = self.model[s][\"actions\"][a][\"q\"]  # current\n",
        "                self.model[s][\"actions\"][a][\"q\"] = old_q + alpha * (r + gamma * exp_q - old_q)  # update\n",
        "                if verbose:\n",
        "                    print(f\"(s,a,r,sN)=({s},{a},{r},{sN}) oldQ={old_q} expQ={exp_q} newQ={self.model[s]['actions'][a]['q']}\")  # trace\n",
        "                self.set_eps_greedy_policy(eps, [s])  # improve behavior at s\n",
        "                s = sN  # advance\n",
        "            if i + 1 == max_el:\n",
        "                break  # cap\n",
        "        print(\"GPI/Expected-SARSA finished.\")  # info"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook we need to pay attention to `policy_eval_td0` that is an implementation of the [TD(0) prediction algorithm](https://bss-osca.github.io/rl/10_td-pred.html#fig-td0-pred-alg)."
      ],
      "metadata": {
        "id": "HeFrNKhq9ewk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elsnCKZDH9ep"
      },
      "source": [
        "## Exercises\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Exercise - A random walk\n",
        "\n",
        "Consider [Exercise 10.6.1](https://bss-osca.github.io/rl/10_td-pred.html#sec-ex-td-pred-random).\n",
        "\n"
      ],
      "metadata": {
        "id": "1BzUCPQxstvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q1\n",
        "\n",
        "Formulate the MDP model and calculate the state-value $v_\\pi$ for each state using the [Bellman equations](https://bss-osca.github.io/rl/08_dp.html#eq-bm-pol-eval). Hint: there is no need to code this. Just solve the Bellman equations for state 2-6.\n",
        "\n",
        "<details>\n",
        "   <summary>Solution (don't look too early)</summary>\n",
        "   The state space is $S = \\{ 1, 2, \\ldots, 6, 7 \\}$ with $A(s) = \\{ \\text{left}, \\text{right}\\}$ (transition to the neighbour states) except for terminating states (1 and 7) which only have an action with transition to itself. Rewards are deterministic $R = \\{0, 1\\}$ which also holds for the transition probabilities. The state-value can be found using the Bellman equations\n",
        "   \n",
        "   $$v_\\pi(s) = \\sum_{a \\in A}\\pi(a | s)\\left( r(s,a) + \\gamma\\sum_{s' \\in S} p(s' | s, a) v_\\pi(s')\\right),$$\n",
        "   \n",
        "   which becomes (with the state-values equal to 0 for the terminating states):\n",
        "   \n",
        "   $$\n",
        "    \\begin{align}\n",
        "    v_\\pi(2) &= 0.5v_\\pi(1) + 0.5v_\\pi(3) = 0.5v_\\pi(3) \\\\\n",
        "    v_\\pi(3) &= 0.5v_\\pi(2) + 0.5v_\\pi( 4 ) \\\\\n",
        "    v_\\pi( 4 ) &= 0.5v_\\pi(3) + 0.5v_\\pi(5) \\\\\n",
        "    v_\\pi(5) &= 0.5v_\\pi( 4 ) + 0.5v_\\pi(6) \\\\\n",
        "    v_\\pi(6) &= 0.5v_\\pi(5) + 0.5(1 + v_\\pi(7)) = 0.5v_\\pi(5) + 0.5\\\\\n",
        "    \\end{align}\n",
        "   $$\n",
        "   \n",
        "   Solving the equations gives state-values $\\frac{1}{6}, \\frac{2}{6}, \\frac{3}{6}, \\frac{4}{6}$ and $\\frac{5}{6}$ for 2-6, respectively.\n",
        "</details>"
      ],
      "metadata": {
        "id": "0rh9O3_0tgAs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Remember state 1 and 7 are terminal states"
      ],
      "metadata": {
        "id": "DbXSSMQzM3ba"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q2\n",
        "\n",
        "Consider the code below, which builds and sets the policy."
      ],
      "metadata": {
        "id": "bbOfHZYjGc0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Build the MDP ---\n",
        "mdp = MDP()\n",
        "\n",
        "# add state keys \"1\"..\"7\"\n",
        "mdp.add_state_space([str(i) for i in range(1, 8)])\n",
        "\n",
        "# add action spaces\n",
        "states = [str(i) for i in range(2, 7)]  # \"2\"..\"6\"\n",
        "for s in states:\n",
        "    mdp.add_action_space(s, [\"left\", \"right\"])\n",
        "mdp.add_action_space(\"1\", [\"dummy\"])\n",
        "mdp.add_action_space(\"7\", [\"dummy\"])\n",
        "\n",
        "# transitions & rewards\n",
        "# right moves\n",
        "for s in range(2, 6):  # 2..5\n",
        "    mdp.add_action(str(s), \"right\", reward=0, pr={str(s + 1): 1.0})\n",
        "mdp.add_action(\"6\", \"right\", reward=1, pr={\"7\": 1.0})  # terminal reward on reaching 7\n",
        "\n",
        "# left moves\n",
        "for s in range(2, 7):  # 2..6\n",
        "    mdp.add_action(str(s), \"left\", reward=0, pr={str(s - 1): 1.0})\n",
        "\n",
        "# dummy self-loops at the ends\n",
        "mdp.add_action(\"1\", \"dummy\", reward=0, pr={\"1\": 1.0})\n",
        "mdp.add_action(\"7\", \"dummy\", reward=0, pr={\"7\": 1.0})\n",
        "\n",
        "# inspect a few states\n",
        "print(\"Actions in 1:\", mdp.get_action_info(\"1\"))\n",
        "print(\"Actions in 2:\", mdp.get_action_info(\"2\"))\n",
        "print(\"Actions in 6:\", mdp.get_action_info(\"6\"))\n",
        "print(\"Actions in 7:\", mdp.get_action_info(\"7\"))\n",
        "\n",
        "# --- Set the policy ---\n",
        "# middle states: 50/50 left/right\n",
        "pi_mid = {\"left\": 0.5, \"right\": 0.5}\n",
        "mdp.set_policy(states, pi_mid)\n",
        "\n",
        "# terminal ends: always dummy\n",
        "pi_ends = {\"dummy\": 1.0}\n",
        "mdp.set_policy([\"1\", \"7\"], pi_ends)\n",
        "\n",
        "print(\"\\nPolicy:\")\n",
        "display(mdp.get_policy())\n",
        "\n"
      ],
      "metadata": {
        "id": "e3tTzSoYG169",
        "outputId": "da9f182e-e986-4870-c2fc-4b51d559a80f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actions in 1: {'dummy': {'r': 0, 'pr': {'1': 1.0}}}\n",
            "Actions in 2: {'left': {'r': 0, 'pr': {'1': 1.0}}, 'right': {'r': 0, 'pr': {'3': 1.0}}}\n",
            "Actions in 6: {'left': {'r': 0, 'pr': {'5': 1.0}}, 'right': {'r': 1, 'pr': {'7': 1.0}}}\n",
            "Actions in 7: {'dummy': {'r': 0, 'pr': {'7': 1.0}}}\n",
            "\n",
            "Policy:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   state action   pr\n",
              "0      1  dummy  1.0\n",
              "1      2   left  0.5\n",
              "2      2  right  0.5\n",
              "3      3   left  0.5\n",
              "4      3  right  0.5\n",
              "5      4   left  0.5\n",
              "6      4  right  0.5\n",
              "7      5   left  0.5\n",
              "8      5  right  0.5\n",
              "9      6   left  0.5\n",
              "10     6  right  0.5\n",
              "11     7  dummy  1.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-48d00d03-ff4f-4909-bfce-19c70da015da\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>state</th>\n",
              "      <th>action</th>\n",
              "      <th>pr</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>dummy</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>left</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>right</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>left</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>right</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>4</td>\n",
              "      <td>left</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>4</td>\n",
              "      <td>right</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>5</td>\n",
              "      <td>left</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>5</td>\n",
              "      <td>right</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>6</td>\n",
              "      <td>left</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>6</td>\n",
              "      <td>right</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>7</td>\n",
              "      <td>dummy</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-48d00d03-ff4f-4909-bfce-19c70da015da')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-48d00d03-ff4f-4909-bfce-19c70da015da button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-48d00d03-ff4f-4909-bfce-19c70da015da');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-550b6beb-bbce-4c67-81b1-4f7937fed74e\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-550b6beb-bbce-4c67-81b1-4f7937fed74e')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-550b6beb-bbce-4c67-81b1-4f7937fed74e button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(mdp\",\n  \"rows\": 12,\n  \"fields\": [\n    {\n      \"column\": \"state\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"1\",\n          \"2\",\n          \"6\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"action\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"dummy\",\n          \"left\",\n          \"right\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pr\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.19462473604038072,\n        \"min\": 0.5,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.5,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try to understand what happens, evaluate the policy and check if the results are the same as found in Question 1."
      ],
      "metadata": {
        "id": "bcxOGaz7EAtj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution\n",
        "\n",
        "# --- Evaluate the policy ---\n",
        "mdp.policy_eval(gamma=1.0)  # undiscounted\n",
        "res = mdp.get_state_values()\n",
        "# Add values from Q1\n",
        "res['v_mdp'] = 0\n",
        "res.set_index('state', inplace=True)\n",
        "res.loc['2', 'v_mdp'] = 1/6\n",
        "res.loc['3', 'v_mdp'] = 2/6\n",
        "res.loc['4', 'v_mdp'] = 3/6\n",
        "res.loc['5', 'v_mdp'] = 4/6\n",
        "res.loc['6', 'v_mdp'] = 5/6\n",
        "print(\"\\nState values:\")\n",
        "display(res)"
      ],
      "metadata": {
        "id": "YSvnFa4fJPCg",
        "outputId": "c5980642-d27a-4e10-d8e6-3029a0c86487",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "State values:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1185897737.py:9: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.16666666666666666' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "              v     v_mdp\n",
              "state                    \n",
              "1      0.000000  0.000000\n",
              "2      0.166648  0.166667\n",
              "3      0.333305  0.333333\n",
              "4      0.499972  0.500000\n",
              "5      0.666645  0.666667\n",
              "6      0.833323  0.833333\n",
              "7      0.000000  0.000000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2dc2b7ae-d138-456d-9eda-79a20567b7a5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>v</th>\n",
              "      <th>v_mdp</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>state</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.166648</td>\n",
              "      <td>0.166667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.333305</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.499972</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.666645</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.833323</td>\n",
              "      <td>0.833333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2dc2b7ae-d138-456d-9eda-79a20567b7a5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2dc2b7ae-d138-456d-9eda-79a20567b7a5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2dc2b7ae-d138-456d-9eda-79a20567b7a5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-38bfd679-62a3-4e26-8733-f31b299648ce\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-38bfd679-62a3-4e26-8733-f31b299648ce')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-38bfd679-62a3-4e26-8733-f31b299648ce button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_c01a45e6-acf5-404f-8f16-45b53a62b7a1\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('res')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_c01a45e6-acf5-404f-8f16-45b53a62b7a1 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('res');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "res",
              "summary": "{\n  \"name\": \"res\",\n  \"rows\": 7,\n  \"fields\": [\n    {\n      \"column\": \"state\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"1\",\n          \"2\",\n          \"6\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"v\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3252941942921188,\n        \"min\": 0.0,\n        \"max\": 0.8333227387415582,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0.0,\n          0.16664783183684406,\n          0.8333227387415582\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"v_mdp\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3253000243161777,\n        \"min\": 0.0,\n        \"max\": 0.8333333333333334,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0.0,\n          0.16666666666666666,\n          0.8333333333333334\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "y4IFzk-0NYHh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q3\n",
        "\n",
        "Consider an episode with sequence $S_0, R_1, S_1, R_2, S_2, R_3, S_3 = 4, 0, 3, 0, 2, 0, 1$. Let the initial state-value estimates of states 2-6 be 0.5 and update the state-values using TD(0) with $\\alpha = 0.1$. It appears that only $V(2)$ changed. Why was only the estimate for this one state changed? By exactly how much was it changed? Hint: There is no need to code this. Just solve equations.\n",
        "\n",
        "*Add your solution*"
      ],
      "metadata": {
        "id": "GbbnH5L6GhLj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* V(4) <- 0.5 + 0.1 (0 + 0.5 - 0.5) = 0.5\n",
        "* V(3) <- 0.5 + 0.1 (0 + 0.5 - 0.5) = 0.5\n",
        "* V(2) <- 0.5 + 0.1 (0 + 0 - 0.5) = 0.45"
      ],
      "metadata": {
        "id": "bscETEylPTEC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q4\n",
        "\n",
        "Consider the TD(0) prediction algorithm with $\\alpha = 0.1$. Make a plot of the state-value estimate (y-axis) given state 2-6 (x-axis) for TD(0) running for 1, 10 and 100 episodes. You may use the code below as a starting point.\n",
        "\n",
        "First, we need an environment representing the problem\n",
        "\n"
      ],
      "metadata": {
        "id": "m1h5wq7EGhlc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title RLEnvRandom\n",
        "\n",
        "from typing import Optional, List, Dict, Any\n",
        "import pandas as pd\n",
        "\n",
        "class RLEnvRandom:\n",
        "    \"\"\"\n",
        "    RL environment for the 1–7 chain:\n",
        "      - States: \"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\"\n",
        "      - Actions in 2–6: \"left\",\"right\"\n",
        "      - Actions in 1 and 7: \"dummy\"\n",
        "      - Reward 1 only for (s=\"6\", a=\"right\") transitioning to \"7\"; otherwise 0\n",
        "      - Terminal after taking \"dummy\" in 1 or 7 (episode ends)\n",
        "    \"\"\"\n",
        "\n",
        "    # --- API mirroring the R version / your agent expectations ---\n",
        "\n",
        "    def get_states(self) -> List[str]:\n",
        "        \"\"\"Return all state keys as strings.\"\"\"\n",
        "        return [str(i) for i in range(1, 8)]\n",
        "\n",
        "    def get_actions(self, s: str) -> List[str] | str:\n",
        "        \"\"\"Return available actions (keys) for a state.\"\"\"\n",
        "        if s in (\"1\", \"7\"):\n",
        "            return \"dummy\"  # to mirror the R return type exactly\n",
        "        return [\"left\", \"right\"]\n",
        "\n",
        "    def get_episode_pi(self, agent: Any, start_state: str) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Generate an episode under the agent's current policy π.\n",
        "        Returns a DataFrame with columns ['s','a','r'].\n",
        "        If start_state is terminal (\"1\" or \"7\"), returns an empty frame.\n",
        "        \"\"\"\n",
        "        # Terminal start → empty episode with correct columns\n",
        "        if start_state in (\"1\", \"7\"):\n",
        "            return pd.DataFrame(columns=[\"s\", \"a\", \"r\"])\n",
        "\n",
        "        s = int(start_state)\n",
        "        rows: List[Dict[str, Any]] = []\n",
        "        max_len = 1000\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            if s in (1, 7):\n",
        "                break  # reached terminal state\n",
        "\n",
        "            a = agent.get_action_pi(str(s))  # sample from π\n",
        "            if a is None:\n",
        "                break  # no available action\n",
        "\n",
        "            # reward only when (s==6 and a==\"right\")\n",
        "            r = 1.0 if (s == 6 and a == \"right\") else 0.0\n",
        "\n",
        "            rows.append({\"s\": str(s), \"a\": a, \"r\": float(r)})\n",
        "\n",
        "            # state transition\n",
        "            if a == \"right\":\n",
        "                s = s + 1\n",
        "            elif a == \"left\":\n",
        "                s = s - 1\n",
        "            elif a == \"dummy\":\n",
        "                # taking dummy at ends would end, but we only offer dummy at 1/7\n",
        "                break\n",
        "\n",
        "        return pd.DataFrame(rows, columns=[\"s\", \"a\", \"r\"])\n",
        "\n",
        "    def get_time_step_data(self, s: str, a: str) -> Dict[str, Optional[str]]:\n",
        "        \"\"\"\n",
        "        One-step transition: return dict with keys:\n",
        "          - 'r': float reward\n",
        "          - 'sN': next state (str) or None if terminal\n",
        "        \"\"\"\n",
        "        s_num = int(s)\n",
        "\n",
        "        # in-chain moves\n",
        "        if a == \"left\" and 1 < s_num < 7:\n",
        "            return {\"r\": 0.0, \"sN\": str(s_num - 1)}\n",
        "\n",
        "        if a == \"right\" and 1 < s_num < 7:\n",
        "            if s_num == 6:\n",
        "                return {\"r\": 1.0, \"sN\": str(s_num + 1)}  # 6 -> 7 with reward 1\n",
        "            return {\"r\": 0.0, \"sN\": str(s_num + 1)}\n",
        "\n",
        "        # terminal dummy action at ends\n",
        "        if s_num in (1, 7) and a == \"dummy\":\n",
        "            return {\"r\": 0.0, \"sN\": None}\n",
        "\n",
        "        raise ValueError(\"Error finding next state and reward!\")\n",
        "\n",
        "# --- quick parity checks\n",
        "env = RLEnvRandom()\n",
        "print(env.get_time_step_data(\"3\", \"right\"))  # {'r': 0.0, 'sN': '4'}\n",
        "print(env.get_time_step_data(\"1\", \"dummy\"))  # {'r': 0.0, 'sN': None}\n",
        "print(env.get_time_step_data(\"6\", \"right\"))  # {'r': 1.0, 'sN': '7'}\n"
      ],
      "metadata": {
        "id": "OZp0gBcqM6dX",
        "cellView": "form",
        "outputId": "58dd78dd-8e5e-476b-d0be-65b3c8890a32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'r': 0.0, 'sN': '4'}\n",
            "{'r': 0.0, 'sN': None}\n",
            "{'r': 1.0, 'sN': '7'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note, we define a method `get_time_step_data` that takes a state and action and returns the reward and next state. This method is used by the RL agent class in method `policy_eval_td0`.\n",
        "\n",
        "We can now define the RL agent and set the policy, which must be done before evaluating a policy:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Je_eStftNULL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = RLAgent()\n",
        "env = RLEnvRandom()\n",
        "\n",
        "# add states\n",
        "agent.add_states(env.get_states())\n",
        "\n",
        "# add actions for each state (normalize string -> list)\n",
        "for s in agent.get_state_keys():\n",
        "    acts = env.get_actions(s)\n",
        "    if isinstance(acts, str):\n",
        "        acts = [acts]\n",
        "    agent.add_actions(s, acts)\n",
        "\n",
        "# --- Set the policy ---\n",
        "# middle states: \"2\"..\"6\" with 50/50 left/right\n",
        "states_mid = [str(i) for i in range(2, 7)]\n",
        "pi_mid = {\"left\": 0.5, \"right\": 0.5}\n",
        "agent.set_policy(states_mid, pi_mid)\n",
        "\n",
        "# ends: always dummy\n",
        "pi_ends = {\"dummy\": 1.0}\n",
        "agent.set_policy([\"1\", \"7\"], pi_ends)\n",
        "\n",
        "# show policy\n",
        "print(agent.get_policy())\n"
      ],
      "metadata": {
        "id": "4mMSl2uGNz8s",
        "outputId": "9e0efa73-c74f-424f-de67-581a23943b5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   state action   pr\n",
            "0      1  dummy  1.0\n",
            "1      2   left  0.5\n",
            "2      2  right  0.5\n",
            "3      3   left  0.5\n",
            "4      3  right  0.5\n",
            "5      4   left  0.5\n",
            "6      4  right  0.5\n",
            "7      5   left  0.5\n",
            "8      5  right  0.5\n",
            "9      6   left  0.5\n",
            "10     6  right  0.5\n",
            "11     7  dummy  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Note a policy must be defined for each possible state that may be generated in an episode. We can now run TD(0) for one episode:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3EmFt_VHOMAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set random seed\n",
        "random.seed(875)\n",
        "np.random.seed(875)\n",
        "\n",
        "# --- initialize state values ---\n",
        "states_mid = [str(i) for i in range(2, 7)]\n",
        "agent.set_state_value(states_mid, 0.5)\n",
        "agent.set_state_value([\"1\", \"7\"], 0.0)\n",
        "\n",
        "print(agent.get_state_values())\n",
        "\n",
        "# --- run one TD(0) evaluation episode starting from state \"4\" ---\n",
        "agent.policy_eval_td0(env, gamma=1.0, states=[\"4\"], max_e=1, reset=False)\n",
        "\n",
        "# --- collect results using dfply ---\n",
        "resTD0 = agent.get_state_values() >> mutate(episodes=1)\n",
        "print(resTD0)\n"
      ],
      "metadata": {
        "id": "H9gRe-xVOij-",
        "outputId": "6815f6f3-eb0e-4934-968e-b2fe2873a29d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  state    v\n",
            "0     1  0.0\n",
            "1     2  0.5\n",
            "2     3  0.5\n",
            "3     4  0.5\n",
            "4     5  0.5\n",
            "5     6  0.5\n",
            "6     7  0.0\n",
            "  state     v  episodes\n",
            "0     1  0.00         1\n",
            "1     2  0.50         1\n",
            "2     3  0.50         1\n",
            "3     4  0.50         1\n",
            "4     5  0.50         1\n",
            "5     6  0.55         1\n",
            "6     7  0.00         1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note, we first have to set the state-values to the default specified. Next, we run TD(0) prediction with one episode `max_e = 1` and starting state 4. You now have to run similar code for 10 and 100 episodes, store the results and plot a line for each result. You may also add the state-values for the MDP for comparison. Finally, comment on your results."
      ],
      "metadata": {
        "id": "OtTDP6dNOyT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution (finish the code)\n",
        "\n",
        "# --- 10 episodes ---\n",
        "\n",
        "resTD0 = resTD0 >> bind_rows(agent.get_state_values() >> mutate(episodes=10))\n",
        "\n",
        "# --- 100 episodes ---\n",
        "resTD0 = resTD0 >> bind_rows(agent.get_state_values() >> mutate(episodes=100))\n",
        "print(resTD0)\n",
        "# --- add MDP values for comparison ---\n",
        "mdp_part = mdp.get_state_values() >> mutate(episodes=\"mdp\")\n",
        "\n",
        "\n",
        "# --- plot (plotnine) ---\n",
        "pt = (ggplot(resTD0, aes(x=\"state\", y=\"v\", color=\"episodes\"))\n",
        "     + geom_line()\n",
        "     + geom_point())\n",
        "pt.show()\n",
        "\n",
        "display(Markdown(\"\"\"\n",
        "Your comments.\n",
        "\"\"\"))"
      ],
      "metadata": {
        "id": "Z2J67hrzP7-M",
        "outputId": "56cd8e36-dd54-4c30-d214-1e6f587e4b2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  state     v  episodes\n",
            "0     1  0.00         1\n",
            "1     2  0.50         1\n",
            "2     3  0.50         1\n",
            "3     4  0.50         1\n",
            "4     5  0.50         1\n",
            "5     6  0.55         1\n",
            "6     7  0.00         1\n",
            "0     1  0.00        10\n",
            "1     2  0.50        10\n",
            "2     3  0.50        10\n",
            "3     4  0.50        10\n",
            "4     5  0.50        10\n",
            "5     6  0.55        10\n",
            "6     7  0.00        10\n",
            "0     1  0.00       100\n",
            "1     2  0.50       100\n",
            "2     3  0.50       100\n",
            "3     4  0.50       100\n",
            "4     5  0.50       100\n",
            "5     6  0.55       100\n",
            "6     7  0.00       100\n",
            "0     1  0.00        10\n",
            "1     2  0.50        10\n",
            "2     3  0.50        10\n",
            "3     4  0.50        10\n",
            "4     5  0.50        10\n",
            "5     6  0.55        10\n",
            "6     7  0.00        10\n",
            "0     1  0.00        10\n",
            "1     2  0.50        10\n",
            "2     3  0.50        10\n",
            "3     4  0.50        10\n",
            "4     5  0.50        10\n",
            "5     6  0.55        10\n",
            "6     7  0.00        10\n",
            "0     1  0.00       100\n",
            "1     2  0.50       100\n",
            "2     3  0.50       100\n",
            "3     4  0.50       100\n",
            "4     5  0.50       100\n",
            "5     6  0.55       100\n",
            "6     7  0.00       100\n",
            "0     1  0.00        10\n",
            "1     2  0.50        10\n",
            "2     3  0.50        10\n",
            "3     4  0.50        10\n",
            "4     5  0.50        10\n",
            "5     6  0.55        10\n",
            "6     7  0.00        10\n",
            "0     1  0.00       100\n",
            "1     2  0.50       100\n",
            "2     3  0.50       100\n",
            "3     4  0.50       100\n",
            "4     5  0.50       100\n",
            "5     6  0.55       100\n",
            "6     7  0.00       100\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n"
          },
          "metadata": {
            "image/png": {
              "width": 640,
              "height": 480
            }
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\nYour comments.\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q5\n",
        "\n",
        "Run an MC prediction algorithm with 1, 10 and 100 episodes. Hint: you have to call `policy_eval_mc` instead of `policy_eval_td0`. Comment on your results.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "b_Y3YBkXDvC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution (finish code)\n",
        "\n",
        "np.random.seed(2947)\n",
        "\n",
        "# convenience\n",
        "states_mid = [str(i) for i in range(2, 7)]  # \"2\"..\"6\"\n",
        "\n",
        "# --- 1 episode ---\n",
        "\n",
        "\n",
        "# --- 10 episodes ---\n",
        "\n",
        "\n",
        "# --- 100 episodes ---\n",
        "\n",
        "\n",
        "# add MDP baseline and keep interior states 2..6\n",
        "\n",
        "# plot\n",
        "pt = (ggplot(resMC, aes(x=\"state\", y=\"v\", color=\"episodes\"))\n",
        "     + geom_line()\n",
        "     + geom_point())\n",
        "pt.show()\n",
        "\n",
        "display(Markdown(\"\"\"\n",
        "Your comments.\n",
        "\"\"\"))"
      ],
      "metadata": {
        "id": "CCQuRhF6h7N6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q6\n",
        "\n",
        "Let us join the results for TD(0) and MC and calculate the root mean square (RMS) error $$\\sqrt{ \\frac{1}{5}\\sum_{s=2,\\ldots 6}(V(s)-v_\\pi(s))^2}$$.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "S9xT7bKJGh2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resTD0 = resTD0 >> mutate(alg=\"TD0\")\n",
        "resMC = resMC >> mutate(alg=\"MC\")\n",
        "resMDP = (mdp.get_state_values() >> mutate(state=X.state.astype(int)))\n",
        "resMDP = resMDP.rename(columns={\"v\": \"vMDP\"})\n",
        "print(resMDP)\n",
        "\n",
        "# dfply pipeline with bind_rows\n",
        "res = (resTD0\n",
        "    >> bind_rows(resMC)\n",
        "    >> filter_by(X.episodes != \"mdp\")\n",
        "    >> left_join(resMDP, by='state')\n",
        "    >> group_by(X.episodes, X.alg)\n",
        "    >> summarize(rms=0.2 * ((X.v - X.vMDP) ** 2).sum())\n",
        ")\n",
        "res['rms'] = [np.sqrt(x) for x in res['rms']]\n",
        "display(res)"
      ],
      "metadata": {
        "id": "HE-d8K66nkRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which algorithm is best in estimating the state-values? Explain.\n",
        "\n",
        "*Add you comments*"
      ],
      "metadata": {
        "id": "OB4N_2ZKvAYW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q7\n",
        "\n",
        "The results are dependent on the value of the step-size parameter. Let us estimate the state-values using TD(0) for $\\alpha = 0.1, 0.2$ and 0.5 and plot the root mean square (RMS) error given the number of episodes:\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8oMWF-OaGh9e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(98)\n",
        "np.random.seed(98)\n",
        "\n",
        "states = [str(i) for i in range(2, 7)]  # \"2\"..\"6\"\n",
        "res = None\n",
        "\n",
        "for run in range(1, 21):  # 1..20\n",
        "    for alpha in (0.1, 0.2, 0.5):\n",
        "        # reset values\n",
        "        agent.set_state_value(states, 0.5)\n",
        "        agent.set_state_value([\"1\", \"7\"], 0.0)\n",
        "\n",
        "        e_old = 0  # number of episiodes\n",
        "        for e in [1] + list(range(5, 101, 5)):  # 1, 5, 10, ...\n",
        "            # run only the additional episodes each step\n",
        "            agent.policy_eval_td0(env, gamma=1.0, states=[\"4\"], max_e=e - e_old, alpha=alpha, reset=False)\n",
        "            e_old = e\n",
        "\n",
        "            step_df = agent.get_state_values() >> mutate(episodes=e, alpha=alpha, run=run)\n",
        "            if res is None:\n",
        "                res = step_df.copy()\n",
        "            else:\n",
        "                res = res >> bind_rows(step_df)\n",
        "res = res >> mutate(state=X.state.astype(int))\n",
        "\n",
        "res_plot = (res\n",
        "            >> left_join(resMDP, by=\"state\")\n",
        "            >> group_by(X.episodes, X.alpha, X.run)\n",
        "            >> summarize(rms=0.2 * ((X.v - X.vMDP) ** 2).sum())\n",
        ")\n",
        "res_plot['rms'] = [np.sqrt(x) for x in res_plot['rms']]\n",
        "res_plot = (res_plot\n",
        "            >> group_by(X.episodes, X.alpha)\n",
        "            >> summarize(rms=X.rms.mean())\n",
        "            >> mutate(alpha_cat=X.alpha.astype(str)))\n",
        "\n",
        "pt = (ggplot(res_plot, aes(x=\"episodes\", y=\"rms\", color=\"alpha_cat\"))\n",
        "     + geom_line())\n",
        "pt.show()\n"
      ],
      "metadata": {
        "id": "6c3-IzD6vwBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain the plot.\n",
        "\n",
        "*Add your comments.*"
      ],
      "metadata": {
        "id": "Of2H2w-VGiEP"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}